{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fingerprint to Zone Classification\n",
    "## Introduction\n",
    "The problem is classification of zone which a user is currently placed in based on some reveiced signals from her mobile phone by the installed sensors (nodes). In this documentation, the designed solution, developed python code, additional notes and analysis and also the results and plots for the problem are given.\n",
    "\n",
    "To solve the problem at first the data should be read from the input file. Also a solution to store and load data in pickle format is provided.\n",
    "\n",
    "The next step is preparing the data for the classification. So data should be divided to separated parts (train & test). Data normalization and also feature reduction has been done in this section. Also some primary analysis on data have been done which can provide insight through it and planning a better strategy to tackle the problem. \n",
    "\n",
    "Based on the properties of the problem and the dataset, three classifiers (Ransom forest, KNN, and SVM) were selected to be test as classifiers. For each classifier, at first multiple parameters of its model was fitted using cross validation on the train data. Afterwards the trained model of classifier were run on the test data and their prediction accuracy was calculated\n",
    "\n",
    "At the end, the Results of running each algorithm on the data set(full data set and subsamplee of 100000 random selected rows), discussion and some proposals for further works on the problem is given.\n",
    " <!--- This a comment --->\n",
    "<!--![image_store_good.jpg](attachment:image_store_good.jpg) --->\n",
    "![image_store_good.jpg](https://raw.githubusercontent.com/aslanmehrabi/Fingerprint_Classifier/master/plot_files/image_store_good.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Input\n",
    "To solve the problem at first the data should be read from the input file. Also a solution to store and load data in pickle format is provided.\n",
    "\n",
    "> Some notes about the provided dataset (TO DATA OWNERS): Unlike the provided document, the input file was not in ascending order of timestamp, as an example in row number 1993 you can ascending pattern is not respected. Also number of nodes(sensors) was 276 (in the provied document it as mentioned as 261 nodes)\n",
    "\n",
    "* Class `ProblemData` is developed for reading, storing and loading dataset.\n",
    "* Function `loadData` will be called to read the data or load from saved pickle file. Also this function can store the input data in a pickle file for faster read data process in next runs. \n",
    "* Function `readInput` can be called by \"loadData\" function to read the data from the input.csv file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy\n",
    "import dill as pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class ProblemData:\n",
    "    defaultSignalVal = 0  # the min amount of received signal (~ zero value)\n",
    "    numNodes = 0 # number of sensors(node)\n",
    "    moment = [] # Time scope of received signal from a suser\n",
    "    signal = [] # Dictionary of received finger prints\n",
    "    sigMat = 0  # Matrix of received fignerprints (dimensions: #row_input * # nodes )\n",
    "    macAdd = [] # array of Mac add of mobile phones\n",
    "    zone = [] # array of zones of users in each line on input\n",
    "    numRows = 0 # total number of rows of input which should be considered\n",
    "\n",
    "    def __init__(self, defaultSignalValue = -100, numNodes = 277):\n",
    "        self.defaultSignalVal = defaultSignalValue\n",
    "        self.numNodes = numNodes\n",
    "\n",
    "    def loadData(self,  useStoredData=False, inputFileName='', storeReadData = 'False',storeDataName='default_store_name', rowReadUntil = -1):\n",
    "        if useStoredData:\n",
    "            storedData = open(storeDataName, 'rb')\n",
    "            self = pickle.load(storedData)  \n",
    "            storedData.close()\n",
    "            return self\n",
    "\n",
    "        else:\n",
    "            if (inputFileName == ''):\n",
    "                print('======> inputFileName field should be passed')\n",
    "                return self\n",
    "\n",
    "            self.readInput(fileName=inputFileName,rowReadUntil=rowReadUntil)  #  To reaad limited number of row\n",
    "            if(storeReadData):\n",
    "                storedData = open(storeDataName, 'wb')\n",
    "                pickle.dump(self, storedData)\n",
    "                storedData.close()\n",
    "            return self\n",
    "        \n",
    "    # read the data from the input file\n",
    "    def readInput(self , fileName, rowReadUntil = -1):\n",
    "        dataAddrMain = fileName\n",
    "        with open(dataAddrMain) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter=',')\n",
    "\n",
    "\n",
    "            if rowReadUntil != -1:\n",
    "                self.numRows = rowReadUntil\n",
    "            else:\n",
    "                self.numRows = sum(1 for row in readCSV) - 1\n",
    "                csvfile.seek(0)  # moving back to the first line of the input\n",
    "\n",
    "            # next(reader) # skip header\n",
    "            for idx, row in enumerate(readCSV):\n",
    "                if idx == 0:\n",
    "                    header = row\n",
    "                else:\n",
    "                    #if(idx % 10000 == 0):\n",
    "                       #print('row reading: ',idx)\n",
    "                    tmpTime = datetime.strptime(row[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "                    self.moment.append(time.mktime(tmpTime.timetuple()))\n",
    "                    self.signal.append(json.loads(row[1].replace(\"'\", \"\\\"\")))  # converting to dict type\n",
    "                    self.macAdd.append(int(row[2]))\n",
    "                    self.zone.append(int(row[3].split(' ')[1]))\n",
    "                    if (idx == self.numRows):  # to read limited number of rows defined by rowReadUntil\n",
    "                        break\n",
    "\n",
    "            self.signal = [dict([int(a), int(x)] for a, x in b.items()) for b in self.signal]  # converting signals to int values \n",
    "            self.signal = [defaultdict(lambda: self.defaultSignalVal, sig) for sig in self.signal]  # add default signal value for probes which did not get any signal\n",
    "            self.sigMat = numpy.full((self.numRows, self.numNodes), self.defaultSignalVal)  # signal matrix: each node is a column\n",
    "            for i in range(self.numRows):\n",
    "                self.sigMat[i][list(self.signal[i].keys())] = list(self.signal[i].values())  \n",
    "\n",
    "            self.zone = numpy.full(self.numRows, self.zone)  # convering zone to numpy Array\n",
    "            self.moment = numpy.full(self.numRows, self.moment) \n",
    "            self.macAdd = numpy.full(self.numRows, self.macAdd)\n",
    "            return self.checkRecievedData()\n",
    "\n",
    "    def checkRecievedData(self):  # can be added to check the validity of read input\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data\n",
    "The next step is preparing the data for the classification. Data should be divided to separated parts (train & test). \n",
    "* An object of `DataPartition` is prepared data for classification which contains feature vector and labels of train and test data.\n",
    "* Function `makeTrainTest` can split data to train and test partitions of specific size defined by user, also load and store feature vectors(if asked by user). Also this function can make a normal distribution of data (if asked by user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as pickle\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "import copy\n",
    "\n",
    "class DataPartition:\n",
    "\n",
    "    fVecTrain = [] # feature vector for train set\n",
    "    fVecTest = [] # feature vector for test set\n",
    "    labelTrain = [] # labels of train set\n",
    "    labelTest = [] # labels of test set\n",
    "    isNormalized = False  # shows whether the data has been normalized\n",
    "\n",
    "\n",
    "# split data to train and test partitions of specific size defined by user + load stores feature vectors(if asked by user) + do data normalization (if asked by user)\n",
    "    def makeTrainTest (self, prblmData, readSampleSize = -1, testPartitionSize = 0.3,randomState = 0 ,doNormalize = False, useSubSample = False, storeSubSample = False, subSamplePcklName = ''):\n",
    "        if useSubSample:\n",
    "            storedData = open(subSamplePcklName, 'rb')  \n",
    "            self = pickle.load(storedData)\n",
    "            storedData.close()\n",
    "            return self\n",
    "\n",
    "        else :\n",
    "            if(readSampleSize == -1):\n",
    "                readSampleSize = prblmData.numRows\n",
    "            sampleSize = min(readSampleSize, prblmData.numRows)  # select sampleSize of random indices of rows\n",
    "\n",
    "\n",
    "            randInd = random.sample(range(prblmData.numRows), sampleSize)\n",
    "            self.isNormalized = doNormalize\n",
    "            if doNormalize :\n",
    "                normalSigMat = StandardScaler().fit_transform(prblmData.sigMat)  \n",
    "                self.fVecTrain, self.fVecTest, self.labelTrain, self.labelTest = train_test_split(normalSigMat[randInd],prblmData.zone[randInd],test_size=testPartitionSize,random_state=randomState)\n",
    "\n",
    "            else:\n",
    "                self.fVecTrain, self.fVecTest, self.labelTrain, self.labelTest = train_test_split(prblmData.sigMat[randInd],prblmData.zone[randInd],test_size=testPartitionSize,random_state=randomState)\n",
    "\n",
    "            if storeSubSample:  # stores train and test data as pickle file\n",
    "                storedData = open(subSamplePcklName, 'wb')\n",
    "                pickle.dump(self, storedData)\n",
    "                storedData.close()\n",
    "\n",
    "        return self\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An engine to run classifier\n",
    "Before defining classifiers, a function which is able to train a classifier on a given train data and apply it on test data is needed. Also this function should be able to provide the accuracy of classifier and other details as well as plotting corresponding plots\n",
    "* Funcion `doClassification` of class `RunClassifier` is defined for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # to load and store data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import dill as pickle\n",
    "\n",
    "class RunClassifier:\n",
    "\n",
    "    # receives a classifier(clf) and set of test and train data and corresponding labels. This function ...\n",
    "    # ... apply the classifier on the data and provides accuracy and related plots of classification. Also ...\n",
    "    # ... it is able to store the trained classifier(with train data) as a pickle file\n",
    "    def doClassification(self, clf, fVecTrain, fVecTest, labelTrain, labelTest, showPlot=False,savePickleModel=False, clfName='clf', dataType = 'data'):\n",
    "        print('clf: %s'%(clf) )\n",
    "        print(\"Model : %s - Data: %s - SampleSize: %d\"%(clfName, dataType,len(labelTrain) + len(labelTest)))\n",
    "        print('learning started')\n",
    "        clf.fit(fVecTrain, labelTrain)\n",
    "        print('model fitted')\n",
    "        prediction = clf.predict(fVecTest)\n",
    "\n",
    "        print('# True pred: ', sum(prediction == labelTest))\n",
    "        print('# False pred: ', sum(prediction != labelTest))\n",
    "\n",
    "        accuracy = accuracy_score(labelTest, prediction)\n",
    "        # print(\"Accuracy :{}\", .format(accuracy_score(labelTest, prediction)))\n",
    "        conf_matrix = confusion_matrix(y_true=labelTest, y_pred=prediction)\n",
    "        print(\"Accuracy : \", accuracy)\n",
    "        print('best parameter: ', clf.best_params_)  # shows the best parametrs of the classifier which was selected by cross validation\n",
    "\n",
    "        if showPlot:\n",
    "            plt.matshow(conf_matrix)\n",
    "            plt.title(\"Model : %s - Data: %s - SampleSize: %d\"%(clfName, dataType,len(labelTrain) + len(labelTest)))\n",
    "            # plt.show()\n",
    "            plt.savefig(clfName + ' ' + dataType +' '+str(len(labelTrain) + len(labelTest)) + ' ' + '.png')\n",
    "\n",
    "        if savePickleModel:  # store pickle file of trained classifier\n",
    "            storedData = open((str(clf).partition('(')[0]) + '_' + str(len(labelTrain) + len(labelTest)) + '.pckl','wb')\n",
    "            pickle.dump(clf, storedData)\n",
    "            storedData.close()\n",
    "\n",
    "        return prediction, accuracy, conf_matrix, clf.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters of the program\n",
    "Before running the main part of program, some parameters should be initialize by the user of the software. These parameters and their description are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "#from ProblemData import ProblemData\n",
    "#from Data_Partition import DataPartition\n",
    "#from RunClassifier import RunClassifier\n",
    "from sklearn import svm, grid_search, datasets\n",
    "\n",
    "inputFileName = 'fingerprints_gt_ver3.csv'\n",
    "\n",
    "readDataUntilRow = -1  # define number of rows which should be read form input file. -1: read the whole data\n",
    "sampleSize = 10000  # define sizse of sample which provides test and train data. -1: readDataUntil will be considered\n",
    "testPartitionSize = 0.2   # size of the partition of the input size which will be separated as test data\n",
    "\n",
    "defaultSignalValue = -100  # default(min) value which a sensor(node) can receive\n",
    "numNodes = 277  # total number of nodes\n",
    "\n",
    "useStoredData = False  # use stores input data as packle file\n",
    "storeReadData = False  # store read data from input to a packle file\n",
    "storeSubSample = False  # store selected subsample as a packle file\n",
    "useSubSample = False   # load stored subsample from packle file\n",
    "savePickleModel = False  # store trained classifier after making the learners [new test data can be run on them immediately]\n",
    "\n",
    "subSamplePcklName = '1000SubSampleVector.pckl'  # name of file to store selected subsample as packle file\n",
    "storeSubSampleName = 'storeSubSample.pckl'  # name of file to load selected subsample as packle file\n",
    "storeDataName = 'store.pckl' # name of file to store input file as a packle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Train Test sets as original form, normalized and feature reduced\n",
    "Feature vector for the input data consists of values of each detected signal by nodes for a single row (fingerprint). In the discussion section the reasons to select this feature vector is provided; also some proposal to improve the feature vectors are mentioned there. Labels for each row of data is the corresponding zone.\n",
    "Here the loaded data is prepared in three formats:\n",
    "* Original Data: Data provided in the input\n",
    "* Normalized Data: Normalization has been done on the original data\n",
    "* Feature reduced: Feature reduction of normalized data using PCA\n",
    "\n",
    "These formats of data will be used as datasets for classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data as prblmData\n",
    "prblmData = ProblemData(defaultSignalValue=defaultSignalValue, numNodes=numNodes)\n",
    "prblmData = prblmData.loadData(useStoredData=useStoredData, inputFileName=inputFileName, storeReadData = storeReadData, storeDataName=storeDataName,\n",
    "                               rowReadUntil=readDataUntilRow)\n",
    "\n",
    "# partitioning data and providing test and train sets and corresponding labels as dataPar \n",
    "dataPar = DataPartition()\n",
    "dataPar = dataPar.makeTrainTest(prblmData=prblmData, readSampleSize=sampleSize, testPartitionSize=testPartitionSize,\n",
    "                                randomState=0, doNormalize=False, useSubSample=useSubSample, storeSubSample=True,\n",
    "                                subSamplePcklName=subSamplePcklName)\n",
    "\n",
    "# providing normalized data of dataPar as dataParNormal\n",
    "dataParNormal = DataPartition()\n",
    "dataParNormal = dataParNormal.makeTrainTest(prblmData=prblmData, readSampleSize=sampleSize,\n",
    "                                            testPartitionSize=testPartitionSize, randomState=0, doNormalize=True,\n",
    "                                            useSubSample=useSubSample, storeSubSample=True,\n",
    "                                            subSamplePcklName=subSamplePcklName)\n",
    "\n",
    "\n",
    "# Feature reduction of normalized data by PCA(with pcaNcomponents dimensions) as dataParPca \n",
    "pcaNcomponent = 10\n",
    "pcaObj = PCA(n_components = pcaNcomponent )\n",
    "fit = pcaObj.fit(dataParNormal.fVecTrain)\n",
    "dataParPca= DataPartition()\n",
    "dataParPca.fVecTrain = pcaObj.fit_transform(dataParNormal.fVecTrain)\n",
    "dataParPca.fVecTest = pcaObj.fit_transform(dataParNormal.fVecTest)\n",
    "dataParPca.labelTrain = dataParNormal.labelTrain\n",
    "dataParPca.labelTest = dataParNormal.labelTest\n",
    "dataParPca.isNormalized = True\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting input data by PCA\n",
    "First and second principle components of applied PCA on data is used to plot all the points and give an insight toward dataset\n",
    "\n",
    ">This figure can be plotted for your selected data if you run the program from the beginning of this file. Also 2D-PCA of sample of 100000 random selected rows of input is shown below:\n",
    "  \n",
    "<!--- >![Pca_100000_samples.png](attachment:Pca_100000_samples.png) --->\n",
    ">![Pca_100000_samples.png](https://raw.githubusercontent.com/aslanmehrabi/Fingerprint_Classifier/master/plot_files/Pca_100000_samples.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting data by first and second principle components of applied PCA on data\n",
    "    plt.plot(dataParPca.fVecTrain[:,0], dataParPca.fVecTrain[:,1], 'b.')\n",
    "    plt.title('2D PCA')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers: \n",
    "Now it is time to determine our classifiers and their hyper parameters. Considering the under considered problem and corresponding datasets, three classifier has been selected to be implemented on the data:\n",
    "* Random forest\n",
    "* K nearest Neighbors (KNN)\n",
    "* Support vector Machines (SVM)\n",
    "\n",
    "Details of how each of these classifiers has been used and tuned for the problem with the corresponding code are given below\n",
    "* parameter cv shows number of partitions for cross validation of hyper parameter tuning\n",
    "* n_jobs = -1 run the data on multiple cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest\n",
    "\n",
    "Random forest with three hyper parameters parameters has been used:\n",
    "* n_estimators: number of trees to make the forest\n",
    "* criteria: measuring quality of a split. “gini” for the Gini impurity and “entropy” for the information gain\n",
    "* max_features: number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_rf = {'n_estimators': [5, 10, 17, 30], 'criterion': ['gini', 'entropy'],'max_features': ['auto', 0.01, 0.1, 0.9], 'n_jobs': [-1]}\n",
    "#paramGrid_rf = {'n_estimators': [30] ,'criterion': ['gini'] ,'n_jobs': [-1]} # n_jobs => runs in parallel\n",
    "clf_rf = GridSearchCV(RandomForestClassifier(), paramGrid_rf, cv=3)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN:\n",
    "KNN with the following parameters has been used:\n",
    "* n_neighbors: number of neighbors to consider\n",
    "* weights: how to weight labels of neighbors. uniform or distance(consider reverse of the neighbors distance)\n",
    "* metric: how to measure the distance: 'minkowski', 'euclidean' or 'manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid_knn = {'n_neighbors': [3, 5, 9, 15], 'weights': ['uniform', 'distance'],'metric': ['minkowski', 'euclidean', 'manhattan']}\n",
    "#paramGrid_knn = {'n_neighbors': [5, 9], 'weights': ['distance'],'metric': ['manhattan']}\n",
    "clf_knn = GridSearchCV(KNeighborsClassifier(algorithm='kd_tree', n_jobs=-1), paramGrid_knn, cv=3) #, verbose=10  # ,scoring='%s_macro' % score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM:\n",
    "SVM with the following parameters has been used:\n",
    "* C : Penalty parameter for miss classification\n",
    "* kernel: used kernel 'linear', 'poly', 'rbf' // rbf is more time consuming but seems to be more concordat to problem\n",
    "* gamma: kernel coefficient. How far the influence of a single training data reaches [low: far / high: close]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {'C': [0.01, 0.1, 1, 10], 'kernel': ['linear', 'poly', 'rbf'], 'gamma': [0.001, 0.01, 0.1, 1]} # rbf is time consuming comparing to others\n",
    "#param_grid_svm = {'C': [0.1], 'kernel': ['rbf'], 'gamma': [0.01]}\n",
    "clf_svm = GridSearchCV(svm.SVC(), param_grid_svm, cv=3) # , verbose=10: write the result of each epoc of cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Classifiers - Fasten your seatbelts!\n",
    "By now, Data has been read, converted to suitable formats (original, normalized, feature reduced) of test and train. \n",
    "Also three classifiers (random forest, KNN, SVM) has been defined with hyper parameters.\n",
    "It is time to run the classifiers on the data. First by cross validation, hyper parameters should be tuned. \n",
    "After that the trained classifiers with selected hyper parameters should be implemented on the test data to determine the classification accuracy and the best classifier.\n",
    "\n",
    "For each classifier and each type of data function `RunClassifier.doClassification` will be called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the classifiers and the sets of data to train the each classifiers on each set of data type\n",
    "clfNames = ['random_forest', 'knn', 'svm']\n",
    "dataTypes = ['original ','normalized','nomalized_PCA']\n",
    "\n",
    "for idx, clf in enumerate([clf_rf , clf_knn, clf_svm]):\n",
    "    for idx2,datap in enumerate([dataPar, dataParNormal, dataParPca]):\n",
    "        runCl = RunClassifier()\n",
    "        prediction, accuracy, conf_matrix, clf.best_params = runCl.doClassification(clf, datap.fVecTrain,datap.fVecTest, datap.labelTrain,datap.labelTest,showPlot=True,savePickleModel=savePickleModel,clfName = clfNames[idx],dataType = dataTypes[idx2])\n",
    "        print('\\n+++++++++++++++++++\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "The mentioned Algorithms (random forest, KNN, SVM) and prepared data types(Original, normalized, feature reduced by PCA in 10 dimension) were run on the input data. Algorithms were run on Subsample of data (100000 rows ) to get the result and using best achieved hyper parameters of this run, algorithms were run on all the dataset (343449 rows). I should note that if a better hardware was available, hyper parameter tuning could be done more accurately for the whole dataset and it would result in improvement of classification accuracy. \n",
    "\n",
    "**As shown below, the best classifier is Random forest with 0.78 accuracy of prediction.** \n",
    "\n",
    "### Run on whole data (343449 rows)\n",
    "| Classifier    | cv-fold       | Data type | Accuracy | Hyper Parameters\n",
    "| -----------   |       ------- |  -----    | ------- |   -----------\n",
    "| Random forest | 3             | original  | __**0.78**__| n_estimators: 10,**30** - criterion: **gini**\n",
    "| Random forest | 3             |   normal  | 0.78     | n_estimators: 10,**30** - criterion: **gini**\n",
    "| KNN           |      3        |   original|  0.71    |  n_neighbors: 5,**9** - weights: **distance** - metric: **manhattan**\n",
    "| KNN           |      3        |   normal  |    0.71  |n_neighbors: 5, **9** - weights: **distance** - metric: **manhattan**\n",
    "| KNN           |      3        |   PCA(10D)|    0.71  |n_neighbors: 5, **9** - weights: **distance** - metric: **manhattan**\n",
    "\n",
    "\n",
    "### Run on subsample (100000 rows)\n",
    "| Classifier    | cv-fold       | Data type | Accuracy | Hyper Parameters\n",
    "| -----------  |        ------- |  -----    | ------- |   -----------\n",
    "| Random forest | 5             | original  |  0.74    | n_estimators:5,10,17,**30** - criterion: **gini**, entropy  - max_features: auto, 0.01, **0.1**, 0.9\n",
    "| Random forest | 5             |   normal  | 0.75     | n_estimators:5,10,17,**30** - criterion: **gini**, entropy  - max_features: auto, 0.01, **0.1**, 0.9\n",
    "| Random forest |      3        |   PCA(10D)|  0.74    |  n_estimators:5,10,17,**30** - criterion: **gini**, entropy\n",
    "| KNN           |      5        |   original|  0.66    |  n_neighbors: 3, **9** - weights: **distance** - metric: **manhattan**\n",
    "| KNN           |      5        |   normal  |    0.66  |n_neighbors: 3, **9** - weights: **distance** - metric: **manhattan**\n",
    "| KNN           |      5        |   PCA(10D)|    0.34  |n_neighbors: 3, **9** - weights: **distance** - metric: **manhattan**\n",
    "| SVM           |      3        |  original |   0.01   | C: **0.1**, gamma: **0.01**, kernel: **rbf**\n",
    "| SVM           |      3        |  PCA(10D) |0.23      | C: **0.1**, gamma: **0.01**, kernel: **rbf**\n",
    "\n",
    "\n",
    "Note that subsampling of size 100000 where generated by random indexing of all rows\n",
    "selected hyper parameters using k-fold cross validation (k = cv-fold) is shown bold in the Hyper parameters column.\n",
    "\n",
    "> Confusion matrix for the best answer (random forest on whole data) is shown below:!\n",
    "<!---![random_forest_original%20_343449.png](attachment:random_forest_original%20_343449.png)--->\n",
    "![random_forest_original%20_343449.png](https://raw.githubusercontent.com/aslanmehrabi/Fingerprint_Classifier/master/plot_files/random_forest_original%20_343449.png)\n",
    "\n",
    "\n",
    "Note that as the problem is multi label classifier, overall precision, recall and F1 score cannot be calculated and each of these parameters can be calculated for each label (one versus all).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
